""" CLIP Model
Adapted from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.
"""
from collections import OrderedDict
import logging
import math
import os
from typing import List, Tuple, Union
import hashlib
import urllib
from tqdm import tqdm
import warnings
import numpy as np
import torch
import torch.nn.functional as F
from torch import nn
from models.utils.simple_tokenizer import SimpleTokenizer
from pkg_resources import packaging
logger = logging.getLogger("IRRA.model")

_MODELS = {
    "RN50": "https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt",
    "RN101": "https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt",
    "RN50x4": "https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt",
    "RN50x16": "https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt",
    "RN50x64": "https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt",
    "ViT-B/32": "https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt",
    "ViT-B/16": "https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt",
    "ViT-L/14": "https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt",
}


def weights_init_kaiming(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')
        nn.init.constant_(m.bias, 0.0)

    elif classname.find('Conv') != -1:
        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')
        if m.bias is not None:
            nn.init.constant_(m.bias, 0.0)
    elif classname.find('BatchNorm') != -1:
        if m.affine:
            nn.init.constant_(m.weight, 1.0)
            nn.init.constant_(m.bias, 0.0)


def weights_init_classifier(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, std=0.001)
        if m.bias:
            nn.init.constant_(m.bias, 0.0)


def available_models() -> List[str]:
    """Returns the names of available CLIP models"""
    return list(_MODELS.keys())


def _download(url: str, root: str):
    os.makedirs(root, exist_ok=True)
    filename = os.path.basename(url)

    expected_sha256 = url.split("/")[-2]
    download_target = os.path.join(root, filename)

    if os.path.exists(download_target) and not os.path.isfile(download_target):
        raise RuntimeError(f"{download_target} exists and is not a regular file")

    if os.path.isfile(download_target):
        if hashlib.sha256(open(download_target, "rb").read()).hexdigest() == expected_sha256:
            return download_target
        else:
            warnings.warn(f"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file")

    with urllib.request.urlopen(url) as source, open(download_target, "wb") as output:
        with tqdm(total=int(source.info().get("Content-Length")), ncols=80, unit='iB', unit_scale=True,
                  unit_divisor=1024) as loop:
            while True:
                buffer = source.read(8192)
                if not buffer:
                    break

                output.write(buffer)
                loop.update(len(buffer))

    if hashlib.sha256(open(download_target, "rb").read()).hexdigest() != expected_sha256:
        raise RuntimeError(f"Model has been downloaded but the SHA256 checksum does not not match")

    return download_target


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1):
        super().__init__()

        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1
        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)

        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)

        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()

        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion)

        self.relu = nn.ReLU(inplace=True)
        self.downsample = None
        self.stride = stride

        if stride > 1 or inplanes != planes * Bottleneck.expansion:
            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1
            self.downsample = nn.Sequential(OrderedDict([
                ("-1", nn.AvgPool2d(stride)),
                ("0", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),
                ("1", nn.BatchNorm2d(planes * self.expansion))
            ]))

    def forward(self, x: torch.Tensor):
        identity = x

        out = self.relu(self.bn1(self.conv1(x)))
        out = self.relu(self.bn2(self.conv2(out)))
        out = self.avgpool(out)
        out = self.bn3(self.conv3(out))

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)
        return out


# class AttentionPool2d(nn.Module):
#     def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):
#         super().__init__()
#         # self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)
#         self.positional_embedding = nn.Parameter(
#             torch.randn((spacial_dim[0] * spacial_dim[1]) + 1, embed_dim) / embed_dim ** 0.5)
#         self.k_proj = nn.Linear(embed_dim, embed_dim)
#         self.q_proj = nn.Linear(embed_dim, embed_dim)
#         self.v_proj = nn.Linear(embed_dim, embed_dim)
#         self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)
#         self.num_heads = num_heads
#
#     def forward(self, x):
#         x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC
#         x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC
#         x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC
#         x, _ = F.multi_head_attention_forward(
#             query=x, key=x, value=x,
#             embed_dim_to_check=x.shape[-1],
#             num_heads=self.num_heads,
#             q_proj_weight=self.q_proj.weight,
#             k_proj_weight=self.k_proj.weight,
#             v_proj_weight=self.v_proj.weight,
#             in_proj_weight=None,
#             in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),
#             bias_k=None,
#             bias_v=None,
#             add_zero_attn=False,
#             dropout_p=0,
#             out_proj_weight=self.c_proj.weight,
#             out_proj_bias=self.c_proj.bias,
#             use_separate_proj_weight=True,
#             training=self.training,
#             need_weights=False
#         )
#
#         return x


class AttentionPool2d(nn.Module):
    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):
        super().__init__()
        # self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)
        self.positional_embedding = nn.Parameter(
            torch.randn((spacial_dim[0] * spacial_dim[1]) + 1, embed_dim) / embed_dim ** 0.5)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)
        self.num_heads = num_heads

        # self.k_proj2 = nn.Linear(embed_dim, embed_dim)
        # self.q_proj2 = nn.Linear(embed_dim, embed_dim)
        # self.v_proj2 = nn.Linear(embed_dim, embed_dim)
        # self.c_proj2 = nn.Linear(embed_dim, output_dim or embed_dim)

        self.t_proj = nn.Linear(output_dim, embed_dim)

        # self.ln_pre_t = LayerNorm(embed_dim)
        # self.ln_pre_i = LayerNorm(embed_dim)
        # self.ln_post = LayerNorm(output_dim)


    def forward(self, x, t=None):
        h, w = x.shape[2], x.shape[3]
        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC
        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (1+HW)NC
        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (1+HW)NC
        if t is None:
            x, _ = F.multi_head_attention_forward(
                query=x, key=x, value=x,
                embed_dim_to_check=x.shape[-1],
                num_heads=self.num_heads,
                q_proj_weight=self.q_proj.weight,
                k_proj_weight=self.k_proj.weight,
                v_proj_weight=self.v_proj.weight,
                in_proj_weight=None,
                in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),
                bias_k=None,
                bias_v=None,
                add_zero_attn=False,
                dropout_p=0,
                out_proj_weight=self.c_proj.weight,
                out_proj_bias=self.c_proj.bias,
                use_separate_proj_weight=True,
                training=self.training,
                need_weights=False
            )
        else:
            # t = self.t_proj(t)
            t = self.t_proj(t).unsqueeze(0)
            # print(t.shape)
            # print(x.shape)
            # input()
            # x = torch.cat([t, x], dim=0)  # mlp(original_text_feat): (1,N,C)
            t, _ = F.multi_head_attention_forward(
                query=t, key=x, value=x,
                embed_dim_to_check=x.shape[-1],
                num_heads=self.num_heads,
                q_proj_weight=self.q_proj.weight,
                k_proj_weight=self.k_proj.weight,
                v_proj_weight=self.v_proj.weight,
                in_proj_weight=None,
                in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),
                bias_k=None,
                bias_v=None,
                add_zero_attn=False,
                dropout_p=0,
                out_proj_weight=self.c_proj.weight,
                out_proj_bias=self.c_proj.bias,
                use_separate_proj_weight=True,
                training=self.training,
                need_weights=False
            )
            x, _ = F.multi_head_attention_forward(
                query=x[0:1], key=x, value=x,
                embed_dim_to_check=x.shape[-1],
                num_heads=self.num_heads,
                q_proj_weight=self.q_proj.weight,
                k_proj_weight=self.k_proj.weight,
                v_proj_weight=self.v_proj.weight,
                in_proj_weight=None,
                in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),
                bias_k=None,
                bias_v=None,
                add_zero_attn=False,
                dropout_p=0,
                out_proj_weight=self.c_proj.weight,
                out_proj_bias=self.c_proj.bias,
                use_separate_proj_weight=True,
                training=self.training,
                need_weights=False
            )
            x = torch.cat([t, x], dim=0)  # (1,N,C) + (1+HW,N,C)
        return x


class ModifiedResNet(nn.Module):
    """
    A ResNet class that is similar to torchvision's but contains the following changes:
    - There are now 3 "stem" convolutions as opposed to 1, with an average pool instead of a max pool.
    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1
    - The final pooling layer is a QKV attention instead of an average pool
    """

    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):
        super().__init__()
        self.output_dim = output_dim
        self.input_resolution = input_resolution

        # the 3-layer stem
        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(width // 2)
        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(width // 2)
        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)
        self.bn3 = nn.BatchNorm2d(width)
        self.avgpool = nn.AvgPool2d(2)
        self.relu = nn.ReLU(inplace=True)

        # residual layers
        self._inplanes = width  # this is a *mutable* variable used during construction
        self.layer1 = self._make_layer(width, layers[0])
        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)
        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)
        self.layer4 = self._make_layer(width * 8, layers[3], stride=1)

        # embed_dim = width * 32  # the ResNet feature dimension
        # spacial_dim = (
        #     input_resolution[0] // 32,
        #     input_resolution[1] // 32,
        # )
        # spacial_dim = input_resolution[0] // 32
        # self.attnpool = AttentionPool2d(spacial_dim, embed_dim, heads, output_dim)
        # self.gap = nn.AdaptiveAvgPool2d(1)
        self.attnpool = AttentionPool2d((input_resolution[0] // 16, input_resolution[1] // 16), width * 32, heads, output_dim)

    def _make_layer(self, planes, blocks, stride=1):
        layers = [Bottleneck(self._inplanes, planes, stride)]

        self._inplanes = planes * Bottleneck.expansion
        for _ in range(1, blocks):
            layers.append(Bottleneck(self._inplanes, planes))

        return nn.Sequential(*layers)

    def forward(self, x, mode=None):
        if mode == 'vid':
            B, T, C, H, W = x.shape
            x = x.view(B * T, C, H, W)
            def stem(x):
                for conv, bn in [(self.conv1, self.bn1), (self.conv2, self.bn2), (self.conv3, self.bn3)]:
                    x = self.relu(bn(conv(x)))
                x = self.avgpool(x)
                return x

            x = x.type(self.conv1.weight.dtype)
            x = stem(x)
            x = self.layer1(x)
            x = self.layer2(x)
            x = self.layer3(x)
            x = self.layer4(x)
            z = x
            # _, c, h, w = z.shape
            # z = z.view(B, T, c, h, w)
            x = self.attnpool(x)
            # hw1, _, c = x.shape
            # x = x.view(hw1, B, T, c)
            # # temporal avg pooling
            # z = z.mean(1)
            # x = x.mean(2)
            # print(z.shape)
            # print(x.shape)
            # input()
            return x, z
        else:
            def stem(x):
                for conv, bn in [(self.conv1, self.bn1), (self.conv2, self.bn2), (self.conv3, self.bn3)]:
                    x = self.relu(bn(conv(x)))
                x = self.avgpool(x)
                return x

            x = x.type(self.conv1.weight.dtype)
            x = stem(x)
            x = self.layer1(x)
            x = self.layer2(x)
            x = self.layer3(x)
            x = self.layer4(x)
            z = x
            x = self.attnpool(x)
            # z = self.gap(z)

            if mode is not None:
                return x, z
                # return x[1:], z
            # return x[1:]
            return x

    def base(self, x):
        def stem(x):
            for conv, bn in [(self.conv1, self.bn1), (self.conv2, self.bn2), (self.conv3, self.bn3)]:
                x = self.relu(bn(conv(x)))
            x = self.avgpool(x)
            return x

        x = x.type(self.conv1.weight.dtype)
        x = stem(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        return x

    def combine(self, x, t):
        x = self.attnpool(x, t)  ## (2,N,C)

        return x


class LayerNorm(nn.LayerNorm):
    """Subclass torch's LayerNorm to handle fp16."""

    def forward(self, x: torch.Tensor):
        orig_type = x.dtype
        ret = super().forward(x.type(torch.float32))
        return ret.type(orig_type)


class QuickGELU(nn.Module):
    def forward(self, x: torch.Tensor):
        return x * torch.sigmoid(1.702 * x)


class ResidualAttentionBlock(nn.Module):
    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):
        super().__init__()

        self.attn = nn.MultiheadAttention(d_model, n_head)
        self.ln_1 = LayerNorm(d_model)
        self.mlp = nn.Sequential(OrderedDict([
            ("c_fc", nn.Linear(d_model, d_model * 4)),
            ("gelu", QuickGELU()),
            ("c_proj", nn.Linear(d_model * 4, d_model))
        ]))
        self.ln_2 = LayerNorm(d_model)
        self.attn_mask = attn_mask

    def attention(self, x: torch.Tensor):
        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None
        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]

    def forward(self, x: torch.Tensor):
        x = x + self.attention(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x


class Transformer(nn.Module):
    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):
        super().__init__()
        self.width = width
        self.layers = layers
        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])

    def forward(self, x: torch.Tensor):
        return self.resblocks(x)


class VisionTransformer(nn.Module):
    def __init__(self, input_resolution: Tuple[int, int], patch_size: int, stride_size: int, width: int, layers: int,
                 heads: int, output_dim: int):
        super().__init__()
        self.input_resolution = input_resolution  # (384, 128)
        self.num_x = (input_resolution[1] - patch_size) // stride_size + 1
        self.num_y = (input_resolution[0] - patch_size) // stride_size + 1
        num_patches = self.num_x * self.num_y

        self.output_dim = output_dim
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=stride_size,
                               bias=False)

        scale = width ** -0.5  # 1/sqrt(768)
        self.class_embedding = nn.Parameter(scale * torch.randn(width))
        self.positional_embedding = nn.Parameter(scale * torch.randn(num_patches + 1, width))
        self.ln_pre = LayerNorm(width)

        self.transformer = Transformer(width, layers, heads)

        self.ln_post = LayerNorm(width)
        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))

    def forward(self, x: torch.Tensor):
        x = self.conv1(x)  # shape = [*, width, grid, grid]
        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]
        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]
        x = torch.cat(
            [self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device),
             x], dim=1)  # shape = [*, grid ** 2 + 1, width]
        x = x + self.positional_embedding.to(x.dtype)
        x = self.ln_pre(x)

        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD

        # x = self.ln_post(x[:, 0, :])
        x = self.ln_post(x)
        z = x

        if self.proj is not None:
            x = x @ self.proj

        return x, z


def tokenize(texts: Union[str, List[str]], tokenizer, context_length: int = 77, truncate: bool = False) -> Union[torch.IntTensor, torch.LongTensor]:
    """
    Returns the tokenized representation of given input string(s)

    Parameters
    ----------
    texts : Union[str, List[str]]
        An input string or a list of input strings to tokenize

    context_length : int
        The context length to use; all CLIP models use 77 as the context length

    truncate: bool
        Whether to truncate the text in case its encoding is longer than the context length

    Returns
    -------
    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length].
    We return LongTensor when torch version is <1.8.0, since older index_select requires indices to be long.
    """
    if isinstance(texts, str):
        texts = [texts]

    sot_token = tokenizer.encoder["<|startoftext|>"]
    eot_token = tokenizer.encoder["<|endoftext|>"]
    all_tokens = [[sot_token] + tokenizer.encode(text) + [eot_token] for text in texts]
    if packaging.version.parse(torch.__version__) < packaging.version.parse("1.8.0"):
        result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)
    else:
        result = torch.zeros(len(all_tokens), context_length, dtype=torch.int)

    for i, tokens in enumerate(all_tokens):
        if len(tokens) > context_length:
            if truncate:
                tokens = tokens[:context_length]
                tokens[-1] = eot_token
            else:
                raise RuntimeError(f"Input {texts[i]} is too long for context length {context_length}")
        result[i, :len(tokens)] = torch.tensor(tokens)
    return result


class PromptLearner(nn.Module):
    def __init__(self, num_class, dataset_name, dtype, token_embedding):
        super().__init__()
        if dataset_name == "VehicleID" or dataset_name == "veri":
            ctx_init = "A photo of a X X X X vehicle."
        else:
            ctx_init = "A photo of a X X X X person."

        ctx_dim = 512
        # use given words to initialize context vectors
        ctx_init = ctx_init.replace("_", " ")
        n_ctx = 4

        tokenizer = SimpleTokenizer()
        tokenized_prompts = tokenize(ctx_init, tokenizer, context_length=77, truncate=True)
        with torch.no_grad():
            embedding = token_embedding(tokenized_prompts).type(dtype)
        self.tokenized_prompts = tokenized_prompts  # torch.Tensor

        n_cls_ctx = 4
        cls_vectors = torch.empty(num_class, n_cls_ctx, ctx_dim, dtype=dtype)
        nn.init.normal_(cls_vectors, std=0.02)
        self.cls_ctx = nn.Parameter(cls_vectors)

        # These token vectors will be saved when in save_model(),
        # but they should be ignored in load_model() as we want to use
        # those computed using the current class names
        self.register_buffer("token_prefix", embedding[:, :n_ctx + 1, :])
        self.register_buffer("token_suffix", embedding[:, n_ctx + 1 + n_cls_ctx:, :])
        self.num_class = num_class
        self.n_cls_ctx = n_cls_ctx

    def forward(self, label):
        cls_ctx = self.cls_ctx[label]
        b = label.shape[0]
        prefix = self.token_prefix.expand(b, -1, -1)
        suffix = self.token_suffix.expand(b, -1, -1)

        prompts = torch.cat(
            [
                prefix,  # (n_cls, 1, dim)
                cls_ctx,  # (n_cls, n_ctx, dim)
                suffix,  # (n_cls, *, dim)
            ],
            dim=1,
        )

        return prompts


class CLIP(nn.Module):
    def __init__(self,
                 embed_dim: int,
                 # vision
                 image_resolution: Union[int, Tuple[int, int]],
                 vision_layers: Union[Tuple[int, int, int, int], int],
                 vision_width: int,
                 vision_patch_size: int,
                 stride_size: int,
                 # text
                 context_length: int,
                 vocab_size: int,
                 transformer_width: int,
                 transformer_heads: int,
                 transformer_layers: int,
                 num_class: int
                 ):
        super().__init__()

        self.context_length = context_length

        if isinstance(vision_layers, (tuple, list)):
            vision_heads = vision_width * 32 // 64
            self.visual = ModifiedResNet(
                layers=vision_layers,
                output_dim=embed_dim,
                heads=vision_heads,
                input_resolution=image_resolution,
                width=vision_width
            )
            self.output_dim = 1024
            self.embed_dim = 2048
            self.in_planes = 2048
            self.in_planes_proj = 1024
        else:
            vision_heads = vision_width // 64
            self.visual = VisionTransformer(
                input_resolution=image_resolution,
                patch_size=vision_patch_size,
                stride_size=stride_size,
                width=vision_width,
                layers=vision_layers,
                heads=vision_heads,
                output_dim=embed_dim
            )
            # self.output_dim = 768
            # self.embed_dim = 768
            # self.in_planes = 768
            # self.in_planes_proj = 768

            self.output_dim = 512
            self.embed_dim = 512
            self.in_planes = 512
            self.in_planes_proj = 512

        self.transformer = Transformer(
            width=transformer_width,
            layers=transformer_layers,
            heads=transformer_heads,
            attn_mask=self.build_attention_mask()
        )

        self.vocab_size = vocab_size
        self.token_embedding = nn.Embedding(vocab_size, transformer_width)
        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))
        self.ln_final = LayerNorm(transformer_width)

        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))
        # self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))

        # clip_feature_dim = 1024
        # projection_dim = 640 * 4
        # hidden_dim = 640 * 8
        #
        # self.text_projection_layer = nn.Linear(clip_feature_dim, projection_dim)
        # self.image_projection_layer = nn.Linear(clip_feature_dim, projection_dim)
        #
        # self.dropout1 = nn.Dropout(0.5)
        # self.dropout2 = nn.Dropout(0.5)
        #
        # self.combiner_layer = nn.Linear(projection_dim * 2, hidden_dim)
        # self.output_layer = nn.Linear(hidden_dim, clip_feature_dim)
        #
        # self.dropout3 = nn.Dropout(0.5)
        #
        # self.dynamic_scalar = nn.Sequential(nn.Linear(projection_dim * 2, hidden_dim), nn.ReLU(), nn.Dropout(0.5),
        #                                     nn.Linear(hidden_dim, 1), nn.Sigmoid())

        # self.dynamic_scalar = nn.Sequential(nn.Linear(1024, 2048),
        #                                     nn.ReLU(),
        #                                     nn.Dropout(0.5),
        #                                     nn.Linear(2048, 1),
        #                                     nn.Sigmoid())

        # tokenizer = SimpleTokenizer()
        # ctx_init = "A photo of a X person."
        # # ctx_init = "A person wearing X."
        # ctx_dim = 512
        # # use given words to initialize context vectors
        # ctx_init = ctx_init.replace("_", " ")
        # n_ctx = 4
        # tokenized_prompts = tokenize(ctx_init, tokenizer, context_length=77, truncate=True)
        # with torch.no_grad():
        #     embedding = self.token_embedding(tokenized_prompts)
        # self.tokenized_prompts = tokenized_prompts.long()  # torch.Tensor
        # n_cls_ctx = 1
        # self.num_class = 1
        # cls_vectors = torch.empty(self.num_class, n_cls_ctx, ctx_dim)
        # nn.init.normal_(cls_vectors, std=0.02)
        # self.cls_ctx = nn.Parameter(cls_vectors)
        #
        # # self.register_buffer("token_prefix", embedding[:, :n_ctx + 1, :])
        # # self.register_buffer("token_suffix", embedding[:, n_ctx + 1 + n_cls_ctx:, :])
        #
        # self.prefix = nn.Parameter(embedding[:, :n_ctx + 1, :].repeat(self.num_class, 1, 1))
        # self.suffix = nn.Parameter(embedding[:, n_ctx + 1 + n_cls_ctx:, :].repeat(self.num_class, 1, 1))
        # self.query = nn.Parameter(cls_vectors)

        # self.num_query = 20
        # self.num_ftrs = 512
        # prefix = torch.Tensor([[49406]] * self.num_query).long()
        # suffix = torch.Tensor([[49407] + [0] * 74] * self.num_query).long()
        # self.prefix = nn.Parameter(self.token_embedding(prefix))
        # self.suffix = nn.Parameter(self.token_embedding(suffix))
        # self.query = nn.Parameter(torch.randn(self.num_query, 1, self.num_ftrs))

        # self.output_dim = 1024
        # self.embed_dim = 2048

        # self.t_proj = nn.Linear(self.output_dim, self.embed_dim)
        # self.c_proj = nn.Linear(self.embed_dim, self.output_dim)

        self.cross_attn = nn.MultiheadAttention(self.output_dim,
                                                self.output_dim // 64,
                                                batch_first=True)

        self.cross_modal_transformer = Transformer(width=self.output_dim,
                                                   layers=1,
                                                   heads=self.output_dim //64)

        # self.ln_pre_t = LayerNorm(self.output_dim)
        # self.ln_pre_i = LayerNorm(self.output_dim)
        # self.ln_post = LayerNorm(self.output_dim)

        scale = self.cross_modal_transformer.width ** -0.5
        proj_std = scale * ((2 * self.cross_modal_transformer.layers) ** -0.5)
        attn_std = scale
        fc_std = (2 * self.cross_modal_transformer.width) ** -0.5

        for block in self.cross_modal_transformer.resblocks:
            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)
            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)
            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)
            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)

        # init cross attn
        nn.init.normal_(self.cross_attn.in_proj_weight, std=attn_std)
        nn.init.normal_(self.cross_attn.out_proj.weight, std=proj_std)

        # self.cross_modal_transformer = CrossTransformer(width=self.output_dim,
        #                                            layers=1,
        #                                            heads=self.output_dim //64)

        # # init residual cross attn
        # for block in self.cross_modal_transformer.resblocks:
        #     nn.init.normal_(block.attn.in_proj_weight, std=attn_std)
        #     nn.init.normal_(block.attn.out_proj.weight, std=proj_std)
        #     nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)
        #     nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)
        #     # nn.init.normal_(block.cross_attn.in_proj_weight, std=attn_std)
        #     # nn.init.normal_(block.cross_attn.out_proj.weight, std=proj_std)

        self.initialize_parameters()

        self.num_classes = num_class
        # self.prompt_learner = PromptLearner(self.num_classes, 'prcc', self.dtype, self.token_embedding)
        #
        # self.classifier = nn.Linear(self.in_planes, self.num_classes, bias=False)
        # self.classifier.apply(weights_init_classifier)
        self.classifier_proj = nn.Linear(self.in_planes_proj, self.num_classes, bias=False)
        self.classifier_proj.apply(weights_init_classifier)

        # self.bottleneck = nn.BatchNorm1d(self.in_planes)
        # self.bottleneck.bias.requires_grad_(False)
        # self.bottleneck.apply(weights_init_kaiming)
        self.bottleneck_proj = nn.BatchNorm1d(self.in_planes_proj)
        self.bottleneck_proj.bias.requires_grad_(False)
        self.bottleneck_proj.apply(weights_init_kaiming)

    def cross_former(self, q, k, v):
        q = q.permute(1, 0, 2)  # NLD -> LND
        q = self.cross_modal_transformer(q)
        q = q.permute(1, 0, 2)  # LND -> NLD

        x = self.cross_attn(q, k, v,
            need_weights=False)[0]

        # x = x.permute(1, 0, 2)  # NLD -> LND
        # x = self.cross_modal_transformer(x)
        # x = x.permute(1, 0, 2)  # LND -> NLD

        # x = self.cross_attn(
        #         self.ln_pre_t(q),
        #         self.ln_pre_i(k),
        #         self.ln_pre_i(v),
        #         need_weights=False)[0]
        # x = self.ln_post(x)

        # x = self.cross_modal_transformer(q, k, v)
        return x

    def initialize_parameters(self):
        nn.init.normal_(self.token_embedding.weight, std=0.02)
        nn.init.normal_(self.positional_embedding, std=0.01)

        if isinstance(self.visual, ModifiedResNet):
            if self.visual.attnpool is not None:
                std = self.visual.attnpool.c_proj.in_features ** -0.5
                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)
                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)
                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)
                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)

            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:
                for name, param in resnet_block.named_parameters():
                    if name.endswith("bn3.weight"):
                        nn.init.zeros_(param)

        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)
        attn_std = self.transformer.width ** -0.5
        fc_std = (2 * self.transformer.width) ** -0.5
        for block in self.transformer.resblocks:
            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)
            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)
            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)
            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)

        if self.text_projection is not None:
            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)

    def build_attention_mask(self):
        # lazily create causal attention mask, with full attention between the vision tokens
        # pytorch uses additive attention mask; fill with -inf
        mask = torch.empty(self.context_length, self.context_length)
        mask.fill_(float("-inf"))
        mask.triu_(1)  # zero out the lower diagonal
        return mask

    @property
    def dtype(self):
        return self.visual.conv1.weight.dtype

    def encode_image(self, image):
        x = self.visual(image.type(self.dtype))
        if isinstance(self.visual, ModifiedResNet):
            # x = self.bottleneck(x[0].float())
            x = x[0].float()
            return x
        else:
            return x[:, 0, :].float()
        # return x

    def encode_text(self, text):
        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]

        x = x + self.positional_embedding.type(self.dtype)
        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD
        x = self.ln_final(x).type(self.dtype)

        # x.shape = [batch_size, n_ctx, transformer.width]
        # take features from the eot embedding (eot_token is the highest number in each sequence)
        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection
        # x = x @ self.text_projection
        return x

    def encode_text_z(self, text):
        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]

        x = x + self.positional_embedding.type(self.dtype)
        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD
        x = self.ln_final(x).type(self.dtype)

        # x.shape = [batch_size, n_ctx, transformer.width]
        # take features from the eot embedding (eot_token is the highest number in each sequence)
        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)]
        # x = x @ self.text_projection
        return x

    def encode_query(self, prompts, tokenized_prompts):
        # x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]

        x = prompts + self.positional_embedding.type(self.dtype)
        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD
        x = self.ln_final(x).type(self.dtype)

        # x.shape = [batch_size, n_ctx, transformer.width]
        # take features from the eot embedding (eot_token is the highest number in each sequence)
        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection
        # x = x @ self.text_projection
        return x

    # def encode_query(self):
    #     # x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]
    #     query_tokens = torch.cat([self.prefix, self.query, self.suffix], dim=1)
    #     x = query_tokens.type(self.dtype)
    #
    #     x = x + self.positional_embedding.type(self.dtype)
    #     x = x.permute(1, 0, 2)  # NLD -> LND
    #     x = self.transformer(x)
    #     x = x.permute(1, 0, 2)  # LND -> NLD
    #     x = self.ln_final(x).type(self.dtype)
    #
    #     # x.shape = [batch_size, n_ctx, transformer.width]
    #     # take features from the eot embedding (eot_token is the highest number in each sequence)
    #     # x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection
    #     # x = x @ self.text_projection
    #     x = x[torch.arange(x.shape[0]), self.tokenized_prompts.argmax(dim=-1)] @ self.text_projection
    #     # x = x[torch.arange(x.shape[0]), 2] @ self.text_projection
    #     return x

    def combine(self, img_feat_before_pooling, t):
        ## Cross Attn
        # t = self.encode_text(text)
        # x = self.visual.combine(img_feat_before_pooling, t)  ## (2,N,C)
        # # t_with_i = x[0]
        # # i = x[1]
        # combined_features = x[0]
        # return combined_features

        # t = self.encode_text(text)
        # t = text
        if isinstance(self.visual, ModifiedResNet):
            t_pro = self.t_proj(t).unsqueeze(1)
            b, c, h, w = img_feat_before_pooling.shape
            img_feat_before_pooling = img_feat_before_pooling.view(b, c, h * w).permute(0, 2, 1)
            combined_features_t = self.cross_former2(t_pro, img_feat_before_pooling, img_feat_before_pooling).view(b, c)
            combined_features = self.c_proj(combined_features_t)
        else:
            # t_pro = t.unsqueeze(1)
            # b, l, c = img_feat_before_pooling.shape
            combined_features = self.cross_former(t, img_feat_before_pooling, img_feat_before_pooling)
        return combined_features

    def encode_vid(self, image):

        x, _ = self.visual(image.type(self.dtype), mode='vid')
        if isinstance(self.visual, ModifiedResNet):
            return x[0].view(-1, 8, 1024).mean(1).float()
            # return self.bottleneck(x[0]).float()
        else:
            return x[:, 0, :].float()
        # return x

    def combine_vid(self, image, text):
        # ## image (B, C, T, H, W)
        # B, C, T, H, W = image.shape
        # image = image.permute(0, 2, 1, 3, 4).view(B*T,C,H,W)

        ## image (B, T, C, H, W)
        # B, T, C, H, W = image.shape
        # image = image.view(B * T, C, H, W)
        if isinstance(self.visual, ModifiedResNet):
            _, image_features_before_pooling = self.visual(image.type(self.dtype), mode='vid')
            # image_features, image_features_before_pooling = x[0].float(), z
            # print(image_features_before_pooling.shape)
            # input()
        else:
            x = self.visual(image.type(self.dtype))
            image_features, image_features_before_pooling = x[:, 0, :].float(),x[:, 0, :].float()

        ## Cross Attn
        t = self.encode_text(text)
        # print(t.shape)
        t_interleave = torch.repeat_interleave(t, repeats=8, dim=0)
        # print(t.shape)
        # i = self.visual(image.type(self.dtype))
        # i = i[0].float()
        # i = self.visual.attnpool(img_feat_before_pooling)
        # i = i[0].float()

        x = self.visual.combine(image_features_before_pooling, t_interleave)  ## (2,N,C)
        # print(x.shape)

        # t_with_i = x[0]
        i = x[1].view(-1, 8, 1024).mean(1)

        combined_features = x[0].view(-1, 8, 1024).mean(1)
        # combined_features = (t+i)/2
        dynamic_scalar = self.dynamic_scalar(combined_features)

        # print(dynamic_scalar)
        # return combined_features + t
        return combined_features + dynamic_scalar * t + (1 - dynamic_scalar) * i


    def combine_encode_vid(self, image, text):
        # ## image (B, C, T, H, W)
        # B, C, T, H, W = image.shape
        # image = image.permute(0, 2, 1, 3, 4).view(B*T,C,H,W)

        ## image (B, T, C, H, W)
        # B, T, C, H, W = image.shape
        # image = image.view(B * T, C, H, W)
        if isinstance(self.visual, ModifiedResNet):
            _, image_features_before_pooling = self.visual(image.type(self.dtype), mode='vid')
            # image_features, image_features_before_pooling = x[0].float(), z
            # print(image_features_before_pooling.shape)
            # input()
        else:
            x = self.visual(image.type(self.dtype))
            image_features, image_features_before_pooling = x[:, 0, :].float(),x[:, 0, :].float()
        # print(image_features_before_pooling.shape)

        ## Cross Attn
        t = self.encode_text(text)
        # print(t.shape)
        t_interleave = torch.repeat_interleave(t, repeats=8, dim=0)
        # print(t.shape)
        # i = self.visual(image.type(self.dtype))
        # i = i[0].float()
        # i = self.visual.attnpool(img_feat_before_pooling)
        # i = i[0].float()

        x = self.visual.combine(image_features_before_pooling, t_interleave)  ## (2,N,C)
        # print(x.shape)

        # t_with_i = x[0]
        i = x[1].view(-1,8,1024).mean(1)

        combined_features = x[0].view(-1,8,1024).mean(1)
        # combined_features = (t+i)/2
        dynamic_scalar = self.dynamic_scalar(combined_features)

        # input()
        # print(dynamic_scalar)
        # return combined_features + t
        return combined_features + dynamic_scalar * t + (1 - dynamic_scalar) * i, i

    def encode_text_irra(self, text):
        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]

        x = x + self.positional_embedding.type(self.dtype)
        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD
        x = self.ln_final(x).type(self.dtype)

        # x.shape = [batch_size, n_ctx, transformer.width]
        # take features from the eot embedding (eot_token is the highest number in each sequence)
        # x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection
        x = x @ self.text_projection
        return x

    def combine_distill(self, img_feat_before_pooling, text):
        ## Cross Attn
        t = self.encode_text(text)
        t_pro = self.t_proj(t).unsqueeze(1)

        b,c,h,w = img_feat_before_pooling.shape
        img_feat_before_pooling = img_feat_before_pooling.view(b,c,h*w).permute(0,2,1)

        # img_feat, img_feat_before_pooling = self.encode_image_feat(images)
        # print(t.shape, img_feat_before_pooling.shape)
        combined_features_t = self.cross_former(t_pro, img_feat_before_pooling, img_feat_before_pooling).view(b,c)
        combined_features_t = self.visual.attnpool.c_proj(combined_features_t)
        # print(combined_features_t.shape)
        # combined_features_t = combined_features_t[torch.arange(combined_features_t.shape[0]), text.argmax(dim=-1)]
        # combined_features_t = combined_features_t[0]
        # print(combined_features_t.shape)
        # input()
        # t = t[torch.arange(combined_features_t.shape[0]), text.argmax(dim=-1)]
        return combined_features_t, t

    def combine_text(self, img_feat_before_pooling, text):
        ## Cross Attn
        # with torch.no_grad():
        t = self.encode_text(text).unsqueeze(0)

        q = self.encode_query().unsqueeze(1)
        # q = torch.mean(q, 0, True)
        # q = q.repeat(img_feat_before_pooling.shape[0], 1)
        q = q.repeat(1, img_feat_before_pooling.shape[0], 1)
        # i = self.visual(image.type(self.dtype))
        # i = i[0].float()
        # i = self.visual.attnpool(img_feat_before_pooling)
        # i = i[0].float()

        # x = self.visual.combine(img_feat_before_pooling, t)  ## (2,N,C)
        # i = x[1]
        # combined_features = x[0]

        combined_features_t = self.visual.combine(img_feat_before_pooling, t)[0]

        x = self.visual.combine(img_feat_before_pooling, q)  ## (2,N,C)
        i = x[self.num_class]
        combined_features = x[:self.num_class]
        combined_features_q = torch.mean(combined_features, 0, False)

        t = t[0]
        return combined_features_q+i, combined_features_t+i, combined_features_t, t

    def combine_display(self, img_feat_before_pooling, text):
        ## Cross Attn
        # with torch.no_grad():
        t = self.encode_text(text)

        q = self.encode_query().unsqueeze(1)
        # q = torch.mean(q, 0, True)
        # q = q.repeat(img_feat_before_pooling.shape[0], 1)
        q = q.repeat(1, img_feat_before_pooling.shape[0], 1)
        # i = self.visual(image.type(self.dtype))
        # i = i[0].float()
        # i = self.visual.attnpool(img_feat_before_pooling)
        # i = i[0].float()
        x = self.visual.combine(img_feat_before_pooling, q)  ## (2,N,C)
        # t_with_i = x[0]
        i = x[self.num_class]
        combined_features = x[:self.num_class]
        # combined_features = torch.mean(combined_features, 0, False)
        # combined_features = (t+i)/2
        # dynamic_scalar = self.dynamic_scalar(combined_features)

        # return combined_features + t
        # return combined_features + dynamic_scalar*q, t, q
        return torch.mean(combined_features, 0, False) + i, t, combined_features, q

    def encode_image_feat(self, image):
        if isinstance(self.visual, ModifiedResNet):
            x, z = self.visual(image.type(self.dtype), mode='feat')
            x = x[0].float()
            # x = self.bottleneck(x[0].float())
            return x, z
        else:
            x = self.visual(image.type(self.dtype))
            return x[:, 0, :].float(),x[:, 0, :].float()

    def forward(self, image, text=None, text_noise=None):
        # image_features = self.encode_image(image)
        # text_features = self.encode_text(text)
        # return image_features, text_features
        if isinstance(self.visual, ModifiedResNet):
            x, z = self.visual(image.type(self.dtype), mode='feat')
            x = x[0].float()
            # if text==None: return x
            # x = self.bottleneck(x[0].float())

            img_feature = nn.functional.avg_pool2d(z, z.shape[2:4]).view(z.shape[0], -1)
            img_feature_proj = x

            # feat = self.bottleneck(img_feature)
            feat = img_feature
            # feat_proj = self.bottleneck_proj(img_feature_proj)
            feat_proj = img_feature_proj
            # feat_proj = None

            if self.training:
                # cls_score = self.classifier(feat)
                cls_score = None
                # cls_score_proj = self.classifier_proj(feat_proj)
                cls_score_proj = None
                if text==None: return [cls_score, cls_score_proj], [feat, feat_proj]
                else:
                    # t = self.encode_text(text)
                    # com_x = self.combine(z, t)
                    # com_x = self.bottleneck_proj(com_x)
                    com_x = None
                    t_bn = None
                    com_z = None
                    t_z_bn = None

                    return [cls_score, cls_score_proj], [feat, feat_proj], [com_x, com_z, t_bn, t_z_bn]
            else:
                # if self.neck_feat == 'after':
                #     # print("Test with feature after BN")
                #     return torch.cat([feat, feat_proj], dim=1)

                if text == None: return torch.cat([feat, feat_proj], dim=1)
                else:
                    t = self.encode_text(text)
                    com_x = self.combine(z, t)
                    com_x = self.bottleneck_proj(com_x)
                    return torch.cat([feat, feat_proj+com_x], dim=1)

        else:
            x, z = self.visual(image.type(self.dtype))

            img_feature = z[:, 0, :]
            img_feature_proj = x[:, 0, :]

            # feat = self.bottleneck(img_feature)
            # feat = img_feature
            feat = None
            feat_proj = self.bottleneck_proj(img_feature_proj)
            # feat_proj = img_feature_proj

            if self.training:
                # cls_score = self.classifier(feat)
                cls_score = None
                cls_score_proj = self.classifier_proj(feat_proj)
                # cls_score_proj = None
                if text==None: return [cls_score, cls_score_proj], [feat, feat_proj]
                else:
                    # t = self.encode_text(text)

                    t = self.encode_text_irra(text)
                    # t_bn = self.bottleneck_proj(t[torch.arange(t.shape[0]), text.argmax(dim=-1)])
                    com_proj = self.combine(x, t)
                    com_proj = com_proj[torch.arange(com_proj.shape[0]), text.argmax(dim=-1)]
                    com_proj = self.bottleneck_proj(com_proj)

                    # t_noise = self.encode_text_irra(text_noise)
                    # # t_bn = self.bottleneck_proj(t[torch.arange(t.shape[0]), text.argmax(dim=-1)])
                    # com_proj_noise = self.combine(x, t_noise)
                    # com_proj_noise = com_proj_noise[torch.arange(com_proj_noise.shape[0]), text_noise.argmax(dim=-1)]
                    # com_z = self.bottleneck_proj(com_proj_noise)

                    # com_proj = None

                    t_bn = None
                    com_z = None
                    t_z_bn = None

                    # label = torch.cat([pids, pids], dim=0)
                    # prompts = self.prompt_learner(label)
                    # prompt_features = self.encode_query(prompts, self.prompt_learner.tokenized_prompts)
                    # # prompt_features = self.encode_text(prompts)
                    # com_p = self.combine(x, prompt_features)
                    # com_p = self.bottleneck_proj(com_p)
                    # com_p = None
                    return [cls_score, cls_score_proj], [feat, feat_proj], [com_proj, com_z, t_bn, t_z_bn]
                        # [com_proj, com_z, t_bn, t_z_bn]
            else:
                # if self.neck_feat == 'after':
                #     # print("Test with feature after BN")
                #     return torch.cat([feat, feat_proj], dim=1)
                if text == None: return torch.cat([feat_proj], dim=1)
                else:
                    # t = self.encode_text(text)
                    # t_bn = self.bottleneck_proj(t)

                    # t_z = self.t_proj(t)
                    # t_z_bn = self.bottleneck(t_z)

                    t = self.encode_text_irra(text)
                    com_x = self.combine(x, t)
                    com_x = com_x[torch.arange(com_x.shape[0]), text.argmax(dim=-1)]
                    com_x = self.bottleneck_proj(com_x)

                    # t_z = self.t_proj(self.encode_text_z(text))
                    # com_z = self.combine2(z, t_z)
                    # com_z = self.bottleneck(com_z)
                    return torch.cat([feat_proj+com_x], dim=1)

    def load_param(self, state_dict):
        # pretrained_dictmodel_dict
        param_dict = {k: v for k, v in state_dict.items() if k in self.state_dict()}

        if 'model' in param_dict:
            param_dict = param_dict['model']
        if 'state_dict' in param_dict:
            param_dict = param_dict['state_dict']
        for k, v in param_dict.items():
            if k == 'visual.positional_embedding' and v.shape != self.visual.positional_embedding.shape:
                v = resize_pos_embed(v, self.visual.positional_embedding, self.visual.num_y, self.visual.num_x)
            elif k == 'positional_embedding' and v.shape != self.positional_embedding.shape:
                v = resize_text_pos_embed(v, self.context_length)
            try:
                self.state_dict()[k].copy_(v)
            except:
                print(f'===========================ERROR occur in copy {k}, {v.shape}=========================')
                print('shape do not match in k :{}: param_dict{} vs self.state_dict(){}'.format(k, v.shape,
                                                                                                self.state_dict()[
                                                                                                    k].shape))

def resize_pos_embed(posemb, posemb_new, hight, width):
    # Rescale the grid of position embeddings when loading from state_dict. Adapted from
    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224
    posemb = posemb.unsqueeze(0)
    posemb_new = posemb_new.unsqueeze(0)

    posemb_token, posemb_grid = posemb[:, :1], posemb[0, 1:]

    gs_old = int(math.sqrt(len(posemb_grid)))
    print('Resized position embedding from size:{} to size: {} with height:{} width: {}'.format(posemb.shape,
                                                                                                posemb_new.shape, hight,
                                                                                                width))
    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)
    posemb_grid = F.interpolate(posemb_grid, size=(hight, width), mode='bilinear')
    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, hight * width, -1)
    posemb = torch.cat([posemb_token, posemb_grid], dim=1)
    return posemb.squeeze(0)


def convert_weights(model: nn.Module):
    """Convert applicable model parameters to fp16"""

    def _convert_weights_to_fp16(l):
        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):
            l.weight.data = l.weight.data.half()
            if l.bias is not None:
                l.bias.data = l.bias.data.half()

        if isinstance(l, nn.MultiheadAttention):
            for attr in [*[f"{s}_proj_weight" for s in ["in", "q", "k", "v"]], "in_proj_bias", "bias_k", "bias_v"]:
                tensor = getattr(l, attr)
                if tensor is not None:
                    tensor.data = tensor.data.half()

        for name in ["text_projection", "proj", "mcq_proj"]:
            if hasattr(l, name):
                attr = getattr(l, name)
                if attr is not None:
                    attr.data = attr.data.half()

    model.apply(_convert_weights_to_fp16)


def build_CLIP_from_openai_pretrained(name: str, image_size: Union[int, Tuple[int, int]], stride_size: int, num_class,
                                      jit: bool = False, download_root: str = None):
    """Load a CLIP model

    Parameters
    ----------
    name : str
        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict

    image_size: Union[int, Tuple[int, int]]
        Input image size, in Re-ID task, image size commonly set to 384x128, instead of 224x224

    jit : bool
        Whether to load the optimized JIT model or more hackable non-JIT model (default).

    download_root: str
        path to download the model files; by default, it uses "~/.cache/clip"

    Returns
    -------
    model : torch.nn.Module
        The CLIP model
    """
    if name in _MODELS:
        model_path = _download(_MODELS[name], download_root or os.path.expanduser("~/.cache/clip"))
    elif os.path.isfile(name):
        model_path = name
    else:
        raise RuntimeError(f"Model {name} not found; available models = {available_models()}")

    try:
        # loading JIT archive
        model = torch.jit.load(model_path, map_location="cpu")
        state_dict = None
    except RuntimeError:
        # loading saved state dict
        if jit:
            warnings.warn(f"File {model_path} is not a JIT archive. Loading as a state dict instead")
            jit = False
        state_dict = torch.load(model_path, map_location="cpu")

    state_dict = state_dict or model.state_dict()

    vit = "visual.proj" in state_dict

    if vit:
        vision_width = state_dict["visual.conv1.weight"].shape[0]
        vision_layers = len(
            [k for k in state_dict.keys() if k.startswith("visual.") and k.endswith(".attn.in_proj_weight")])
        vision_patch_size = state_dict["visual.conv1.weight"].shape[-1]
        grid_size = round((state_dict["visual.positional_embedding"].shape[0] - 1) ** 0.5)
        image_resolution = vision_patch_size * grid_size
    else:
        counts: list = [len(set(k.split(".")[2] for k in state_dict if k.startswith(f"visual.layer{b}"))) for b in
                        [1, 2, 3, 4]]
        vision_layers = tuple(counts)
        vision_width = state_dict["visual.layer1.0.conv1.weight"].shape[0]
        output_width = round((state_dict["visual.attnpool.positional_embedding"].shape[0] - 1) ** 0.5)
        vision_patch_size = None
        assert output_width ** 2 + 1 == state_dict["visual.attnpool.positional_embedding"].shape[0]
        image_resolution = output_width * 32

    embed_dim = state_dict["text_projection"].shape[1]
    context_length = state_dict["positional_embedding"].shape[0]
    vocab_size = state_dict["token_embedding.weight"].shape[0]
    transformer_width = state_dict["ln_final.weight"].shape[0]
    transformer_heads = transformer_width // 64
    transformer_layers = len(set(k.split(".")[2] for k in state_dict if k.startswith(f"transformer.resblocks")))

    # print(embed_dim)

    model_cfg = {
        'embed_dim': embed_dim,
        'image_resolution': image_resolution,
        'vision_layers': vision_layers,
        'vision_width': vision_width,
        'vision_patch_size': vision_patch_size,
        'context_length': context_length,
        'vocab_size': vocab_size,
        'transformer_width': transformer_width,
        'transformer_heads': transformer_heads,
        'transformer_layers': transformer_layers,
        'num_class': num_class,
    }

    # modify image resolution to adapt Re-ID task
    model_cfg['image_resolution'] = image_size
    model_cfg['stride_size'] = stride_size
    logger.info(f"Load pretrained {name} CLIP model with model config: {model_cfg}")
    model = CLIP(**model_cfg)

    # covert model to fp16
    # convert_weights(model)

    # resize modified pos embedding
    model.load_param(state_dict)
    return model, model_cfg
